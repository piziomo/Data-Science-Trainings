{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piziomo/Data-Science-Trainings/blob/main/Lab_2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHRUpS2jG_Wi"
      },
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHZp6G33G_Wn"
      },
      "source": [
        "# Lab: Missing data {#sec-pandas-transforming-data}\n",
        "\n",
        "In these exercises, we have provided \"good\" datasets where data is properly structured in rows and columns, there's no missing data, or mixed types... Unfortunately, in real-world cases this may not be as common as we may think of. One of the main issues we may encounter is that some data may be missing.\n",
        "\n",
        "This can be due to a number of reasons i.e., because the measurements failed during a specific period of time, because criteria changed... and the implications of this can be serious depending on the proportion of missing data and how we deal with it. Unfortunately there's not a single, correct way of dealing with missing data: some times we may want to get rid of them if they are anecdotal, sometimes we may want to infer their values based on existing data...\n",
        "\n",
        "In this notebook we will be working with a modified version of the _The Office_ dataset that we have been using, where some values have been removed.\n",
        "\n",
        "\n",
        "## Assessing Missing values\n",
        "\n",
        "In this case we will be reading the modified dataset stored in `office_ratings_missing.csv` where some values have been removed. The first thing we may want to do is to know where and how many of those values have been removed to inform what to do next with them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLcw7tccG_Wp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "df = pd.read_csv('data/raw/office_ratings_missing.csv', encoding = 'UTF-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuvEyO6IG_Wq"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AXF8YWlG_Wr"
      },
      "source": [
        "Did you notice something weird on the row number 1?\n",
        "\n",
        "\n",
        "::: callout-important\n",
        "\n",
        "### Storing missing values\n",
        "\n",
        "At this stage, it is important to understand how Python stores missing values. For python, any missing data will be represented as any of these values: `NaN` (Not a Number), `NA` (Not Available) or `None`. This is to differenciate that with cases where we may see a \"gap\" in the dataframe that may look like a missing data, but it is an empty string instead (`\"\"`) or a string with a white space (`\" \"`). These are not considered empty values.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "Regretfully, exploring the head or the tail of the dataframe may not be a good idea, especially in large datasets. We may want to use `info` instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPMKvLcuG_Ws"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_ARPkJvG_Ws"
      },
      "source": [
        "We are missing values in our `imdb_rating` and `total_votes` columns. Now, we need to know how many values are missing. We can do that in multiple ways. One is to combine the method `.isna` , which returns either `True` (`1`) or `False` (`2`), and then sum the values that are true (and thus, are null):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBnP-nmqG_Wt"
      },
      "outputs": [],
      "source": [
        "# Count missing values\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhzcq0GkG_Wu"
      },
      "source": [
        "Another method is to get the maximum possible values and substract the sum of existing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUTrh2_jG_Wv"
      },
      "outputs": [],
      "source": [
        "# Count missing values\n",
        "df.shape[0] - df.count()\n",
        "# df.shape returns the size of the dataframe and count() will only count the rows where there is a value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmj0DUeUG_Wv"
      },
      "source": [
        "Now we have an understanding of where the issues are and how many are there. Now the question is: _What to do with missing data?_\n",
        "\n",
        "## Inputting values{#sec-inputting-missing-data}\n",
        "\n",
        "A quick solution is to replace missing values with either `0` or give them a roughtly central value (the mean).\n",
        "\n",
        "To do this we use the `fillna` method, which _fills_ any missing values (`NA` -Not Available or `NaN` ), in this case with a `0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIjhOtKaG_Ww"
      },
      "outputs": [],
      "source": [
        "df['imdb_rating_with_0'] = df['imdb_rating'].fillna(0)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTQNE8gMG_Ww"
      },
      "source": [
        "This worked! `imdb_rating_with_0` does not have any missing value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnq94uFhG_Ww"
      },
      "source": [
        "::: callout-important\n",
        "\n",
        "In the above, we are saving the \"fixed\" (in more technical terms, \"imputed\") data column in `imdb_rating` in a new column named `imdb_rating_with_0`.\n",
        "It is a good practice to keep the \"original\" data and make \"copies\" of data features where you have made changes.\n",
        "In this way, you can always go back and do things differently and also compare the data sets with and without the intervention (for instance, in the following, we will be able to compare the descriptive statistics of the two columns)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "Please note, that **we are significantly altering some statistical values**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w24InVFG_Wx"
      },
      "outputs": [],
      "source": [
        "df[['imdb_rating', 'imdb_rating_with_0']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5vPItAxG_Wx"
      },
      "source": [
        "In order to try to avoid that, we can fill the missing with the mean. Let's compare the statistical values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdZHOhaUG_Wx"
      },
      "outputs": [],
      "source": [
        "# Create a new column where we assign missing values with the mean.\n",
        "df['imdb_rating_with_mean'] = df['imdb_rating'].fillna(df['imdb_rating'].mean())\n",
        "\n",
        "# Compare the outputs\n",
        "df[['imdb_rating', 'imdb_rating_with_0', 'imdb_rating_with_mean']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wci1zE2RG_Wy"
      },
      "source": [
        "We can plot these to see what looks most reasonable (you can probably also make an educated guess here)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M2DiHKBG_Wy"
      },
      "outputs": [],
      "source": [
        "df['imdb_rating_with_mean'].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4js3hCLG_Wy"
      },
      "outputs": [],
      "source": [
        "df['imdb_rating_with_0'].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6RxI8L9G_Wy"
      },
      "source": [
        "Going with the mean seems quite sensible in this case. Especially as the data is gaussian so the mean is probably an accurate represenation of the central value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S6StUZeG_Wz"
      },
      "outputs": [],
      "source": [
        "# Create a histogram\n",
        "ax = df['imdb_rating'].hist()\n",
        "# Plot the histogram and add a vertical line on the mean\n",
        "ax.axvline(df['imdb_rating'].mean(), color='k', linestyle='--')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5RfpEAlG_Wz"
      },
      "source": [
        "## Transformations\n",
        "\n",
        "Some statistical models, such as standard linear regression, require the predicted variable to be gaussian distributed (a single central point and a roughly symmetrical decrease in frequency, see [this Wolfram alpha page](https://www.wolframalpha.com/input/?i=gaussian+0+1).\n",
        "\n",
        "The distribution of votes is positively skewed (most values are low)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84g52MWqG_Wz"
      },
      "outputs": [],
      "source": [
        "df['total_votes'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_usBQ9OPG_Wz"
      },
      "source": [
        "A log transformation can make this data closer to a gaussian distributed data variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll-nYwRqG_W0"
      },
      "source": [
        "::: callout-caution\n",
        "\n",
        "## `numpy` Library\n",
        "\n",
        "We are now brining in another library called [`numpy`](https://numpy.org/) (numerical python) -- see here:\n",
        "\n",
        "`numpy` is a central library that most of data analysis protocols make use of. Pandas makes regular use of Numpy for instance. Several of the underlying data structures such as arrays, indexes we use in Pandas and elsewhere will build on Numpy structures. Don't worry too much about it for now. We'll use `numpy` more later -- Here is a useful user guide to give you an idea: <https://numpy.org/doc/stable/user/index.html#user>\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44WpFQIDG_W0"
      },
      "source": [
        "For the log transformation we are going to use `log2` method provided by `numpy` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVKUz9n1G_W0"
      },
      "outputs": [],
      "source": [
        "# This is how we import numpy, usually the convention is to use np as the short variable name and you can access numpy functions by `np.`\n",
        "import numpy as np\n",
        "\n",
        "df['total_votes_log'] = np.log2(df['total_votes'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxExzHIRG_W0"
      },
      "outputs": [],
      "source": [
        "df['total_votes_log'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk8RNQ1KG_W1"
      },
      "source": [
        "That is less skewed, but not ideal. Perhaps a _square root_ transformation instead?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgwfchZ3G_W1"
      },
      "outputs": [],
      "source": [
        "df['total_votes_sqrt'] = np.sqrt(df['total_votes'])\n",
        "df['total_votes_sqrt'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kriHUvu7G_W1"
      },
      "source": [
        "...well, maybe a inverse/reciprocal transformation. It is possible we have hit the limit on what we can do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjGEFojLG_W1"
      },
      "outputs": [],
      "source": [
        "df['total_votes_recip'] = np.reciprocal(df['total_votes'])\n",
        "df['total_votes_recip'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3B8JX2hG_W2"
      },
      "source": [
        "At this point, I think we should concede that we can make the distribution less positively skewed. However, transformations are not magic and we cannot turn a heavily positively skewed distribution into a normally distributed one.\n",
        "\n",
        "Oh well.\n",
        "\n",
        "We can calculate `z` scores though so we can plot both `total_votes` and `imdb_ratings` on a single plot. Currently, the IMDB scores vary between 0 and 10 whereas the number of votes number in the thousands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeoAiyKMG_W2"
      },
      "outputs": [],
      "source": [
        "df['total_votes_z'] = (df['total_votes'] - df['total_votes'].mean()) / df['total_votes'].std()\n",
        "df['imdb_rating_z'] = (df['imdb_rating'] - df['imdb_rating'].mean()) / df['imdb_rating'].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-tE1uJVG_W2"
      },
      "outputs": [],
      "source": [
        "df['total_votes_z'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ0HM7GhG_W2"
      },
      "outputs": [],
      "source": [
        "df['imdb_rating_z'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_NP7LnKG_W3"
      },
      "source": [
        "## Visualising data with matplotlib\n",
        "\n",
        "Now we can compare the trends in score and number of votes on a single plot.\n",
        "\n",
        "::: callout-warning\n",
        "We are going to use a slightly different approach to creating the plots. Called to the `plot()` method from Pandas actually use a library called `matplotlib`. We are going to use the `pyplot` module of `matplotlib` directly.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smLtClWsG_W3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQO9rVPSG_W_"
      },
      "source": [
        "Convert the `air_date` into a datetime object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbkx-2Y1G_W_"
      },
      "outputs": [],
      "source": [
        "df['air_date'] =  pd.to_datetime(df['air_date'], dayfirst=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rLk_OFfG_W_"
      },
      "source": [
        "Then call the subplots function from pyplot to create two plots. From this we take the two plot axis (`ax1`, `ax2`) and call the method `scatter` for each to plot `imdb_rating_z` and `total_votes_z`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vlsnit2AG_W_"
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "ax1.scatter( df['air_date'], df['imdb_rating_z'], color = 'red')\n",
        "ax1.set_title('IMDB rating')\n",
        "ax2.scatter( df['air_date'], df['total_votes_z'], color = 'blue')\n",
        "ax2.set_title('Total votes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0z-z_weG_XA"
      },
      "source": [
        "We can do better than that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtGSbOzaG_XA"
      },
      "outputs": [],
      "source": [
        "plt.scatter(df['air_date'], df['imdb_rating_z'], color = 'red', alpha = 0.1)\n",
        "plt.scatter(df['air_date'], df['total_votes_z'], color = 'blue', alpha = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWmmcwAkG_XA"
      },
      "source": [
        "We have done a lot so far. Exploring data in part 1, plotting data with the inbuilt Pandas methods in part 2 and dealing with both missing data and transfromations in part 3.\n",
        "\n",
        "In part 4, we will look at creating your own functions, a plotting library called seaborn and introduce a larger dataset."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}